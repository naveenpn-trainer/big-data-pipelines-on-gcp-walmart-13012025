# Big Data Foundations on GCP

> Big Data refers to the data which is large, fast and complex type of structured, semi-structured and unstructured data generated from a variety of different sources, which becomes difficult to **Store and Process** using a traditional system.

**Challenges of Big Data**

1. Storage : Distributed Storage System
2. Processing :  Massive Parallel Processing (MPP)

## Hadoop 2.x (Distributed Storage and Processing Framework)

> Apache Hadoop is a software framework that allows us to store and process large datasets in parallel and distributed fashion

Hadoop consists of 3 main components

1. Storage Layer
2. Resource Management Layer
3. Data Processing Layer

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf5-TEqJeOXvcqgn3wvB6U13zIZVXDvZlq5Wa34hDwcJYEMPrBRSB0gOr2vz1cINfUCtrVtGzkpVm5TePVSehTmHNV0LsTM27M0N2BQoLvHICZ-bURPUfOczFnIhbkkgQgfnlGyb733acGmWNJk2a0WkL4E?key=Lcjgu0sLjm8U8i3A_14gRg)

## Daemon Services

1. NameNode
2. SecondaryNameNode
3. DataNode
4. ResourceManager
5. NodeManager



## Apache Spark

> Apache Spark is an in-memory cluster computing framework designed for big data workloads

1. High Performance Batch Computation
2. Real-time streaming (Structured Streaming, Kafka Streams, Flink)
3. Machine Learning Analytics
4. Graph Computation
5. Data Integration and ETL

* Apache Spark is natively written using Scala 

## PySpark

> PySpark is a python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdKcVAk--7497uqqC9Qwz_Hy9DtsxBtzqOqxrwg7nYUSQ7_iGiFSW8gNIM0Y9wFKtac3n6u_Jt-HJFU4dU45rVTOcmxjuyB7cA31EcXT9seje8CFMbMmPUbHRZjL85iyKBrAiBTfianwQ0LAxCvSbFdrNc?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfjqgMz5XY-5WwqmXcvIMBS6f4-XXqU2Wow-8Vtda9MnOw8Xtwy_xkp8VlMRsi7wjtmX0A8A89uJDA0R75dBO9ZxocpQjaPUbVXoISd08Uf7b-XvgesObUCsu_X179AQ50Dn7tfppnlLsTStxg7mXmAFYg?key=uvmlVet7-pBAx-jz0PuzLA)

## RDD's (Deprecated)



## Spark SQL

> Spark SQL is one of the module in the Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfcJrT6iWiUpQAH44OuMuz98aD_CYFhEQEndKzvSVBs7LObfwCWZsjVlRWT5efpr6PkJnTi-zZc7vkWszkBY3SIfGiEAtVtxABmMaC0rYbbePlubnJRiU2Xz8TvNsHFLY48K7t8pYh1jQiq68r6NvpcdYk?key=yGW25KMloT80Lch6YWjT9A)

In any spark application, you perform three steps

1. Load the data
2. Process
3. Store 

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcLnVClkZgE2cimmXyPoEQghDvWlOrveNAajIEj1abX4iOd1YucmKVCjUjNHmZCluieprkNDN3XlQ2gXeg0UnMrxaf8ff9FlOEI56OcG9apIstl4hIGLQFF6BjE2ZnejX388LKpYEPllaJJWyuyeIwny-Yp?key=yGW25KMloT80Lch6YWjT9A)



```python
from pyspark.sql.functions import col
input_path = "gs://vernon1234567890/UserDataset/users_001.csv"
output_path = "gs://vernon1234567890/UserDataset/output_json"
df = spark.read.json(path=input_path,header=True,inferSchema=True)
#df.show()
result_df = df.filter(col("country")=="India")
result_df.write.mode("overwrite").format("json").save(output_path)
```



















